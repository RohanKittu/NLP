{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpora :- is the collection of all the similar kind of text which are preset in a document.All the similar document together called as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unicode_samples', 'indian', 'stopwords', 'brown', 'swadesh', 'mac_morpho', 'conll2002.zip', 'indian.zip', 'abc', 'comparative_sentences', 'brown_tei.zip', 'cmudict.zip', 'conll2000.zip', 'universal_treebanks_v20.zip', 'words', 'pros_cons', 'udhr2', 'lin_thesaurus', 'webtext', 'smultron.zip', 'names', 'sentiwordnet', 'dolch.zip', 'wordnet_ic.zip', 'brown.zip', 'alpino.zip', 'panlex_swadesh.zip', 'cmudict', 'sinica_treebank.zip', 'treebank.zip', 'ptb', 'inaugural', 'ppattach.zip', 'dependency_treebank.zip', 'opinion_lexicon.zip', 'cess_esp.zip', 'product_reviews_2', 'genesis.zip', 'reuters.zip', 'conll2007.zip', 'conll2002', 'comparative_sentences.zip', 'switchboard.zip', 'cess_cat.zip', 'udhr.zip', 'subjectivity.zip', 'pl196x.zip', 'ieer', 'problem_reports', 'timit.zip', 'floresta', 'paradigms.zip', 'gazetteers.zip', 'wordnet.zip', 'inaugural.zip', 'sinica_treebank', 'stopwords.zip', 'verbnet.zip', 'gutenberg', 'ieer.zip', 'ycoe.zip', 'shakespeare.zip', 'sentence_polarity', 'framenet_v17.zip', 'kimmo.zip', 'chat80.zip', 'kimmo', 'qc.zip', 'senseval', 'verbnet', 'udhr2.zip', 'senseval.zip', 'chat80', 'framenet_v15.zip', 'unicode_samples.zip', 'biocreative_ppi', 'framenet_v17', 'words.zip', 'pil', 'alpino', 'omw', 'cess_cat', 'shakespeare', 'city_database', 'product_reviews_2.zip', 'abc.zip', 'europarl_raw', 'sentiwordnet.zip', 'rte.zip', 'movie_reviews', 'toolbox.zip', 'product_reviews_1.zip', 'omw.zip', 'jeita.zip', 'wordnet_ic', 'names.zip', 'conll2000', 'dependency_treebank', 'floresta.zip', 'nombank.1.0.zip', 'wordnet', 'cess_esp', 'ptb.zip', 'mac_morpho.zip', 'knbc.zip', 'opinion_lexicon', 'toolbox', 'comtrans.zip', 'swadesh.zip', 'propbank.zip', 'gutenberg.zip', 'product_reviews_1', 'twitter_samples.zip', 'treebank', 'state_union.zip', 'machado.zip', 'rte', 'nps_chat', 'crubadan', 'semcor.zip', 'biocreative_ppi.zip', 'ppattach', 'europarl_raw.zip', 'switchboard', 'brown_tei', 'verbnet3.zip', 'verbnet3', 'crubadan.zip', 'pil.zip', 'ycoe', 'webtext.zip', 'sentence_polarity.zip', 'timit', 'pl196x', 'nps_chat.zip', 'state_union', 'city_database.zip', 'subjectivity', 'framenet_v15', 'masc_tagged.zip', 'paradigms', 'genesis', 'gazetteers', 'twitter_samples', 'qc', 'lin_thesaurus.zip', 'udhr', 'movie_reviews.zip', 'dolch', 'problem_reports.zip', 'smultron', 'pros_cons.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the list of zip file present in the corpora ,where each zip file contains many folders and files which are related to each other.when ever we want to analyse any text ,The program compare these text to the existing corpora and extract information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This command is used to see all the files in the zip file of corpus\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "#This command is used to see the data in the file of the zipfile in the corpus\n",
    "nltk.corpus.gutenberg.words('whitman-leaves.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LeavesofGrassbyWaltWhitman1855]Come,saidmysoul,SuchversesformyBodyletuswrite,(forweareone,)ThatshouldIafterreturn,Or,long,longhence,inotherspheres,Theretosomegroupofmatesthechantsresuming,(TallyingEarth'ssoil,trees,winds,tumultuouswaves,)Everwithpleas'dsmileImaykeepon,Everandeveryettheversesowning--as,first,IhereandnowSigningforSoulandBody,settothemmyname,WaltWhitman[BOOKI.INSCRIPTIONS]}One's-SelfISingOne's-selfIsing,asimpleseparateperson,YetutterthewordDemocratic,thewordEn-Masse.OfphysiologyfromtoptotoeIsing,NotphysiognomyalonenorbrainaloneisworthyfortheMuse,IsaytheFormcompleteisworthierfar,TheFemaleequallywiththeMaleIsing.OfLifeimmenseinpassion,pulse,andpower,Cheerful,forfreestactionform'dunderthelawsdivine,TheModernManIsing.}AsIPonder'dinSilenceAsIponder'dinsilence,Returninguponmypoems,considering,lingeringlong,APhantomarosebeforemewithdistrustfulaspect,Terribleinbeauty,age,andpower,Thegeniusofpoetsofoldlands,Astomedirectinglikeflameitseyes,Withfingerpointingtomanyimmortalsongs,Andmenacingvoice,Whatsingestthou?itsaid,Know'stthounotthereishutonethemeforever-enduringbards?AndthatisthethemeofWar,thefortuneofbattles,Themakingofperfectsoldiers.Beitso,thenIanswer'd,ItoohaughtyShadealsosingwar,andalongerandgreateronethanany,Wagedinmybookwithvaryingfortune,withflight,advanceandretreat,victorydeferr'dandwavering,(Yetmethinkscertain,orasgoodascertain,atthelast,)thefieldtheworld,Forlifeanddeath,fortheBodyandfortheeternalSoul,Lo,Itooamcome,chantingthechantofbattles,Iaboveallpromotebravesoldiers.}InCabin'dShipsatSeaIncabin'dshipsatsea,Theboundlessblueoneverysideexpanding,Withwhistlingwindsandmusicofthewaves,thelargeimperiouswaves,Orsomelonebarkbuoy'donthedensemarine,Wherejoyousfulloffaith,spreadingwhitesails,Shecleaves"
     ]
    }
   ],
   "source": [
    "#printing all the words preset in the file\n",
    "x=nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
    "for i in x[:500]:\n",
    "    print(i,sep=',',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai=\"\"\"sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".\n",
    "\n",
    "As machines become increasingly pable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.\n",
    "\n",
    "Artificial intelligence can be classified into three different types of systems: analytical, human-inspired, and humanized artificial intelligence. Analytical AI has only characteristics consistent with cognitive intelligence; generating cognitive representation of the world and using learning based on past experience to inform future decisions. Human-inspired AI has elements from cognitive and emotional intelligence; understanding human emotions, in addition to cognitive elements, and considering them in their decision making. Humanized AI shows characteristics of all types of competencies (i.e., cognitive, emotional, and social intelligence), is able to be self-conscious and is self-aware in interactions with others.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sometimes',\n",
       " 'called',\n",
       " 'machine',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'is',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'by',\n",
       " 'humans',\n",
       " '.',\n",
       " 'Colloquially',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " '``',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'is',\n",
       " 'often',\n",
       " 'used',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'machines',\n",
       " '(',\n",
       " 'or',\n",
       " 'computers',\n",
       " ')',\n",
       " 'that',\n",
       " 'mimic',\n",
       " '``',\n",
       " 'cognitive',\n",
       " \"''\",\n",
       " 'functions',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'associate',\n",
       " 'with',\n",
       " 'the',\n",
       " 'human',\n",
       " 'mind',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " '``',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'and',\n",
       " '``',\n",
       " 'problem',\n",
       " 'solving',\n",
       " \"''\",\n",
       " '.',\n",
       " 'As',\n",
       " 'machines',\n",
       " 'become',\n",
       " 'increasingly',\n",
       " 'pable',\n",
       " ',',\n",
       " 'tasks',\n",
       " 'considered',\n",
       " 'to',\n",
       " 'require',\n",
       " '``',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'are',\n",
       " 'often',\n",
       " 'removed',\n",
       " 'from',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'a',\n",
       " 'phenomenon',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'AI',\n",
       " 'effect',\n",
       " '.',\n",
       " 'A',\n",
       " 'quip',\n",
       " 'in',\n",
       " 'Tesler',\n",
       " \"'s\",\n",
       " 'Theorem',\n",
       " 'says',\n",
       " '``',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'whatever',\n",
       " 'has',\n",
       " \"n't\",\n",
       " 'been',\n",
       " 'done',\n",
       " 'yet',\n",
       " '.',\n",
       " \"''\",\n",
       " 'For',\n",
       " 'instance',\n",
       " ',',\n",
       " 'optical',\n",
       " 'character',\n",
       " 'recognition',\n",
       " 'is',\n",
       " 'frequently',\n",
       " 'excluded',\n",
       " 'from',\n",
       " 'things',\n",
       " 'considered',\n",
       " 'to',\n",
       " 'be',\n",
       " 'AI',\n",
       " ',',\n",
       " 'having',\n",
       " 'become',\n",
       " 'a',\n",
       " 'routine',\n",
       " 'technology',\n",
       " '.',\n",
       " 'Modern',\n",
       " 'machine',\n",
       " 'capabilities',\n",
       " 'generally',\n",
       " 'classified',\n",
       " 'as',\n",
       " 'AI',\n",
       " 'include',\n",
       " 'successfully',\n",
       " 'understanding',\n",
       " 'human',\n",
       " 'speech',\n",
       " ',',\n",
       " 'competing',\n",
       " 'at',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'level',\n",
       " 'in',\n",
       " 'strategic',\n",
       " 'game',\n",
       " 'systems',\n",
       " '(',\n",
       " 'such',\n",
       " 'as',\n",
       " 'chess',\n",
       " 'and',\n",
       " 'Go',\n",
       " ')',\n",
       " ',',\n",
       " 'autonomously',\n",
       " 'operating',\n",
       " 'cars',\n",
       " ',',\n",
       " 'intelligent',\n",
       " 'routing',\n",
       " 'in',\n",
       " 'content',\n",
       " 'delivery',\n",
       " 'networks',\n",
       " ',',\n",
       " 'and',\n",
       " 'military',\n",
       " 'simulations',\n",
       " '.',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " 'can',\n",
       " 'be',\n",
       " 'classified',\n",
       " 'into',\n",
       " 'three',\n",
       " 'different',\n",
       " 'types',\n",
       " 'of',\n",
       " 'systems',\n",
       " ':',\n",
       " 'analytical',\n",
       " ',',\n",
       " 'human-inspired',\n",
       " ',',\n",
       " 'and',\n",
       " 'humanized',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'Analytical',\n",
       " 'AI',\n",
       " 'has',\n",
       " 'only',\n",
       " 'characteristics',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'cognitive',\n",
       " 'intelligence',\n",
       " ';',\n",
       " 'generating',\n",
       " 'cognitive',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'and',\n",
       " 'using',\n",
       " 'learning',\n",
       " 'based',\n",
       " 'on',\n",
       " 'past',\n",
       " 'experience',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'future',\n",
       " 'decisions',\n",
       " '.',\n",
       " 'Human-inspired',\n",
       " 'AI',\n",
       " 'has',\n",
       " 'elements',\n",
       " 'from',\n",
       " 'cognitive',\n",
       " 'and',\n",
       " 'emotional',\n",
       " 'intelligence',\n",
       " ';',\n",
       " 'understanding',\n",
       " 'human',\n",
       " 'emotions',\n",
       " ',',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'cognitive',\n",
       " 'elements',\n",
       " ',',\n",
       " 'and',\n",
       " 'considering',\n",
       " 'them',\n",
       " 'in',\n",
       " 'their',\n",
       " 'decision',\n",
       " 'making',\n",
       " '.',\n",
       " 'Humanized',\n",
       " 'AI',\n",
       " 'shows',\n",
       " 'characteristics',\n",
       " 'of',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'competencies',\n",
       " '(',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'cognitive',\n",
       " ',',\n",
       " 'emotional',\n",
       " ',',\n",
       " 'and',\n",
       " 'social',\n",
       " 'intelligence',\n",
       " ')',\n",
       " ',',\n",
       " 'is',\n",
       " 'able',\n",
       " 'to',\n",
       " 'be',\n",
       " 'self-conscious',\n",
       " 'and',\n",
       " 'is',\n",
       " 'self-aware',\n",
       " 'in',\n",
       " 'interactions',\n",
       " 'with',\n",
       " 'others',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "ai_tokens=word_tokenize(ai)\n",
    "ai_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 290 distint tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 20, 'intelligence': 10, '.': 10, 'and': 9, 'ai': 8, 'in': 7, 'to': 7, 'the': 7, 'is': 6, '``': 6, ...})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making all the words to lower case.\n",
    "for i in ai_tokens:\n",
    "    fdist[i.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 20),\n",
       " ('intelligence', 10),\n",
       " ('.', 10),\n",
       " ('and', 9),\n",
       " ('ai', 8),\n",
       " ('in', 7),\n",
       " ('to', 7),\n",
       " ('the', 7),\n",
       " ('is', 6),\n",
       " ('``', 6)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 most frequently occured words in the para\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "len(blankline_tokenize(ai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'most',\n",
       " 'beautiful',\n",
       " 'thing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'even',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'touched',\n",
       " ',',\n",
       " 'it',\n",
       " 'can',\n",
       " 'only',\n",
       " 'be',\n",
       " 'felt',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"The most beautiful thing in the world can not be even seen or touched,it can only be felt.\"\n",
    "to=nltk.word_tokenize(string)\n",
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'most'),\n",
       " ('most', 'beautiful'),\n",
       " ('beautiful', 'thing'),\n",
       " ('thing', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'even'),\n",
       " ('even', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'touched'),\n",
       " ('touched', ','),\n",
       " (',', 'it'),\n",
       " ('it', 'can'),\n",
       " ('can', 'only'),\n",
       " ('only', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forming bigram\n",
    "list(nltk.bigrams(to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'most', 'beautiful'),\n",
       " ('most', 'beautiful', 'thing'),\n",
       " ('beautiful', 'thing', 'in'),\n",
       " ('thing', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'can'),\n",
       " ('world', 'can', 'not'),\n",
       " ('can', 'not', 'be'),\n",
       " ('not', 'be', 'even'),\n",
       " ('be', 'even', 'seen'),\n",
       " ('even', 'seen', 'or'),\n",
       " ('seen', 'or', 'touched'),\n",
       " ('or', 'touched', ','),\n",
       " ('touched', ',', 'it'),\n",
       " (',', 'it', 'can'),\n",
       " ('it', 'can', 'only'),\n",
       " ('can', 'only', 'be'),\n",
       " ('only', 'be', 'felt'),\n",
       " ('be', 'felt', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trigrams\n",
    "list(nltk.trigrams(to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'most', 'beautiful', 'thing', 'in'),\n",
       " ('most', 'beautiful', 'thing', 'in', 'the'),\n",
       " ('beautiful', 'thing', 'in', 'the', 'world'),\n",
       " ('thing', 'in', 'the', 'world', 'can'),\n",
       " ('in', 'the', 'world', 'can', 'not'),\n",
       " ('the', 'world', 'can', 'not', 'be'),\n",
       " ('world', 'can', 'not', 'be', 'even'),\n",
       " ('can', 'not', 'be', 'even', 'seen'),\n",
       " ('not', 'be', 'even', 'seen', 'or'),\n",
       " ('be', 'even', 'seen', 'or', 'touched'),\n",
       " ('even', 'seen', 'or', 'touched', ','),\n",
       " ('seen', 'or', 'touched', ',', 'it'),\n",
       " ('or', 'touched', ',', 'it', 'can'),\n",
       " ('touched', ',', 'it', 'can', 'only'),\n",
       " (',', 'it', 'can', 'only', 'be'),\n",
       " ('it', 'can', 'only', 'be', 'felt'),\n",
       " ('can', 'only', 'be', 'felt', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ngrams\n",
    "list(nltk.ngrams(to,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave:gave\n",
      "give:give\n",
      "given:given\n",
      "giving:give\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"gave\",\"give\",\"given\",\"giving\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+ps.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is more aggressive stemmer as compare to porter stemmer.\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave:gav\n",
      "give:giv\n",
      "given:giv\n",
      "giving:giv\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"gave\",\"give\",\"given\",\"giving\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+ls.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note :- There are lots of temmers present ,we should wisely use these stemmers for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_len=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave:gave\n",
      "give:give\n",
      "given:given\n",
      "giving:giving\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=[\"gave\",\"give\",\"given\",\"giving\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+word_len.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizer have not changed word ,this because we did not give any pos tags here.It has assumed all the words as nouns.The pos tags telles you about what exactly the given word is ,whether its a noun,pronoun,adjective ..etc.POS standards for parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some basic words which we do not need in our analysis is know as stop words.The nltk has its own set of stop words which we are going to see belowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[-.\"\"]', re.UNICODE)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.compile(r'[-.\"\"'']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function by the name non_teen_sum. Given 3 integer values, a,b,c, it should return their sum. However, if any of the three values is a teen, then that value counts as 0, except 15 and 16:\n",
    "examples:\n",
    "- non_teen_sum(13,3,30) --> 33\n",
    "- non_teen_sum(3,16,9) --> 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_teen_sum(a,b,c):\n",
    "    lis=list([a,b,c])\n",
    "    buff=[]\n",
    "    for i in lis:\n",
    "        if(i in list(range(10,21))):\n",
    "            if(i==15 or i==16):\n",
    "                buff.append(True)\n",
    "            else:\n",
    "                buff.append(False)\n",
    "        else:\n",
    "            buff.append(True)\n",
    "    count=0\n",
    "    for i in buff:\n",
    "        if(i==False):\n",
    "            count+=1\n",
    "    if(count>1):\n",
    "        return 0\n",
    "    else:\n",
    "        return (a+b+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def non_teen_sum(a,b,c):\n",
    "    lis=[10,11,12,13,14,17,18,19,10]\n",
    "    l = [a,b,c]\n",
    "    buff=True\n",
    "    for i in l:\n",
    "        if(i in lis):\n",
    "            buff = False\n",
    "            return 0\n",
    "    if(buff==True):\n",
    "        return (a+b+c)\n",
    "    \n",
    "non_teen_sum(11,23,45)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    if(x in [10,11,12,13,14,17,18,19,20]):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "def non_teen_sum(a,b,c):\n",
    "    l=[a,b,c]\n",
    "    a=map(l,g)\n",
    "    return sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-38b7cf107459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnon_teen_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m54\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-293fcb650002>\u001b[0m in \u001b[0;36mnon_teen_sum\u001b[0;34m(a, b, c)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnon_teen_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "non_teen_sum(11,23,54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-591c261afe8e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-591c261afe8e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def sdsd(a,b,c)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def sdsd(a,b,c)\n",
    "lis = [1,2,3]\n",
    "sum(list(map(fdf,lis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdf(x):\n",
    "    if(x in [10,11,12,13,14,17,18,19,20]):\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
